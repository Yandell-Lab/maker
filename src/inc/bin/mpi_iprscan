#!\usr\bin\perl

use warnings;
use strict "vars";
use strict "refs";

use FindBin;
use lib "$FindBin::Bin/../lib";
use lib "$FindBin::Bin/../perl/lib";
use vars qw($RANK $LOG $EXE $TMP $LOCK @CMD_ARGS $VERSION);
use Config;
use File::Which;

BEGIN{
   $VERSION = '2.10';
   @CMD_ARGS = @ARGV;

   #find iprscan
   $EXE = File::Which::which("iprscan");
   if (! $EXE) {
       die "ERROR: You must have iprscan installed and in you path to use mpi_iprscan\n";
   }

   #build temp directory
   my $n = umask();
   umask(0000);
   $TMP = "/tmp/iprscanTMP";
   mkdir($TMP, 07777) if(! -d $TMP);
   mkdir("$TMP/tmp", 07777) if(! -d "$TMP/tmp");
   umask($n);

   #what to do on ^C
   $SIG{'INT'} = sub {
      print STDERR "\n\nmpi_iprscan aborted by user!!\n\n";
      exit (2);
   };

   #supress warnings from storable module
   $SIG{'__WARN__'} = sub {
      warn $_[0] if ( $_[0] !~ /Not a CODE reference/ &&
		      $_[0] !~ /Can\'t store item CODE/ &&
		      $_[0] !~ /Find\:\:skip_pattern|File\/Find\.pm/
		    );
   };

   #output to log file of seq that caused rank to die
   $SIG{'__DIE__'} =
   sub {
      if (defined ($LOG) && defined $_[0]) {
	 my $die_count = $LOG->get_die_count();
	 $die_count++;

	 $LOG->add_entry("DIED","RANK",$RANK);
	 $LOG->add_entry("DIED","COUNT",$die_count);
      }

      die $_[0];
   };
}

use Cwd;
use Storable qw(freeze thaw);
use FileHandle;
use File::Copy;
use File::Path;
use Getopt::Long qw(:config pass_through);
use File::Temp qw(tempfile tempdir);
use Iterator::Fasta;
use Fasta;
use iprscan::runlog;
use ds_utility;
use Error qw(:try);
use Error::Simple;
use Perl::Unsafe::Signals;
use Process::IPRchunk;
use Process::MpiTiers;
use Parallel::MPIcar qw(:most);
use Proc::Signal;

#--MPI_Init requires there to be arguments on @ARGV
#--This is a logic problem by the Package Authors
#--This is a hack to solve the problem
if (not @ARGV) {
   push (@ARGV, 'null');
   MPI_Init();			#initiate the MPI
   shift @ARGV;
}
else {
   MPI_Init();			#initiate the MPI
}

select((select(STDOUT), $|=1)[0]); #make STDOUT buffer flush immediately

#no usage, I just print the usage statement from iprscan
#my $usage = ''

#-------------------------------------------------------------------------------
#------------------------------------ MAIN -------------------------------------
#-------------------------------------------------------------------------------

#--set object variables for serialization of data
$Storable::forgive_me = 1; #allows serializaion of objects with code refs

#------INITIATE MPI VARIABLES------
my $rank = MPI_Comm_rank(MPI_COMM_WORLD); #my proccess number
my $size = MPI_Comm_size(MPI_COMM_WORLD); #how many proccesses
$RANK = $rank;

#MPI SIGNAL CODES
#--mpi message tags
my $who_I_am       = 1111;
my $what_I_want    = 2222;
my $result_status  = 3333;
my $request_status = 4444;
my $c_res_status   = 5555;
my $c_req_status   = 6666;
my $chunk_status   = 7777;
my $work_order     = 8888; #generic data tag
my $mpi_data       = 9999;
my $mpi_list       = 1212;
my $message_length = 1313;
my $message_stat   = 1414;

#--message_stat type signals
my $message_ok = 0;

#--what_I_want type signals
my $need_tier   = 1;
my $need_helper = 2;
my $have_c_res  = 3;
my $need_c_res  = 4;

#--request_status signals
my $wait_as_helper = 5;
my $yes_tier       = 6;
my $yes_helper     = 7;
my $no_helper      = 8;
my $go_chunk       = 9;
my $reset          = 10;
my $wait_ask_again = 11;
my $terminate      = 12;

#--c_req_status signals
my $go_chunk       = 13;
my $c_reset        = 14;
my $c_terminate    = 15;

#--results_status signals
my $yes_result = 16;
my $no_result  = 17;

#--c_res_status signal
my $yes_c_res      = 18;
my $no_c_res      = 19;

#--chunk_status signals
my $yes_chunk = 20;
my $no_chunk  = 21;

#---variables for thread and the root node
my @c_results;
my @failed;
my @interrupted;
my @res_loc;
my @limbo_stack;
my @active;
my @chunks;
my @returned_chunks;
my $t_need_flag;
my $t_tier;
my $t_tier_result;
my $t_chunk;
my $t_chunk_result;
my $t_terminate;
my $empty;

#---global variables
my %OPT;
my @appl;
my $root = 0; #define root node (only changed for debugging)
$OPT{retry} = 2;
$OPT{datastore} = 1;
$OPT{always_try} = 1;
$OPT{min_contig} = 5;
$OPT{seqtype} = 'p'; #default

#---Process options on the command line
try{
    GetOptions("i=s"       => \$OPT{infile},
               "o=s"       => \$OPT{outfile},
               "appl=s"    => \@appl,
               "nocrc"     => \$OPT{nocrc},
               "seqtype=s" => \$OPT{seqtype},
               "trtable=i" => \$OPT{trtable},
               "goterms"   => \$OPT{goterms},
               "iprlookup" => \$OPT{iprlookup},
               "format=s"  => \$OPT{format},
               "verbose"   => \$OPT{verbose},
               "retry"     => \$OPT{retry}, #hidden option for retrying
               "chpc"      => \$OPT{chpc}, #hidden option for local dbs
               "TMP"       => \$TMP,   #hidden option o specify TMP
               "cli"       => \$OPT{cli}, #just used to strip off the option
	       "debug"     => \$main::debug,
	       "no_threads" => \$main::no_threads,
               "version" => sub{print "$VERSION\n" if($rank == $root); MPI_Finalize(); exit(0)},
               );

    $main::quiet = 1 unless($OPT{verbose}); #suppress status messages
    $main::no_threads = 1 if(!$Config::Config{useithreads});

    #collect app list from iprscan if not suplied by user
    if(! @appl){
        my $conf = Cwd::abs_path($EXE);
        $conf =~ s/[^\/]+$//;
        $conf .= "../conf/iprscan.conf";
        die "ERROR: Cannot find iprscan.conf\n" if(! -e $conf);

        open(my $IN, "< $conf");
        my @data = <$IN>;
        close($IN);

        my ($apps) = grep {/^applications\=/} @data;
        $apps =~ s/^applications\=//;
        chomp($apps);
        @appl = split(',', $apps);
        die "ERROR: Cannot determine default applications\n" if(! @appl);
    }

    #apply apps to OPT
    $OPT{appl} = join(",", @appl);
    $OPT{_appl} = \@appl;

    #get current working directory
    $OPT{CWD} = Cwd::cwd();

    #build out_base and out_name for datastore
    $OPT{out_name} = $OPT{infile} || 'data';
    $OPT{out_name} =~ /([^\/]+)$/;
    $OPT{out_name} = $1;
    $OPT{out_name} =~ s/(.+)\.[^\.]+$/$1/;
    $OPT{out_base} = Cwd::cwd()."/$OPT{out_name}.iprscan.output";

    #build localized iprscan database for each node
    if($OPT{chpc} && ! -e "$TMP/iprscan/data/ok"){
        my $lock = new File::NFSLock("$TMP/.iprscan_lock", 'EX', 1200, 1205);

        #build local db in /tmp
        if(! -e "$TMP/iprscan/data/ok" && $lock){
            #remove old failed directories
            if(-d "$TMP/iprscan"){
                move("$TMP/iprscan", "$TMP/old");
		File::Path::rmtree("$TMP/old");
            }
            if(-d "$TMP/test"){
                move("$TMP/test", "$TMP/old");
		File::Path::rmtree("$TMP/old");
            }

            my $free = `df /tmp | grep -v \"Filesystem\" | awk \'{print \$4}\'`;
            if($free > 16000000){
                my @tar_db = split (":", $ENV{'TAR_DB'});
                @tar_db = grep {-d $_} @tar_db;

                if (@tar_db){
                    my $db = $tar_db[int(rand(@tar_db))];

                    my @files = ('latest_match.tar.gz',
                                 'latest_pthr.tar.gz',
                                 'latest_nopthr.tar.gz');

                    if(-e "$db/$files[0]" &&
                       -e "$db/$files[1]" &&
                       -e "$db/$files[1]"
                       ){

                        my $fail;
                        foreach my $file (@files){
                            last if($fail);
                            mkdir("$TMP/test") unless(-d "$TMP/test");
                            copy("$db/$file", "$TMP/test");
                            system("cd $TMP/test\n".
                                   "tar -zxvmf $file"
                                   );

                            $fail = $?;

                            unlink("$TMP/test/$file");
                        }

                        if($fail){
			    File::Path::rmtree("$TMP/test");
			  }
                        else{
                            move("$TMP/test/iprscan", "$TMP/iprscan");
			    File::Path::rmtree("$TMP/test");
                            system("index_data.pl -p $TMP/iprscan/data -inx -bin");
                            system("touch $TMP/iprscan/data/ok");
                        }
                    }
                }
            }
        }

        $lock->unlock if($lock);
    }
}
catch Error::Simple with{
    my $E = shift;

    print STDERR $E->{-text};
    die "\n\nmpi_iprscan failed parsing command line options!!\n\n";
};

#---set up thread support
unless($main::no_threads){
    require threads;
    require threads::shared;
    threads::shared->import();

    unless($threads::VERSION >= 1.67){
	die "mpi_iprscan requires threads version 1.67 or greater\n",
	"You have version ". $threads::VERSION ."\n";
    }

    #--change signal handling for threads
    #what to do on ^C
    $SIG{'INT'} = sub {
	print STDERR "\n\nmpi_iprscan aborted by user!!\n\n";
	unless($main::no_threads){
	    my @threads = threads->list();
	    foreach my $thr (@threads){
		$thr->kill('KILL')->join();
	    }
	}
	exit (2);
    };

   #output to log file of seq that caused rank to die
   $SIG{'__DIE__'} = sub {
       if (defined ($LOG) && defined $_[0]) {
	   my $die_count = $LOG->get_die_count();
	   $die_count++;
	   
	   $LOG->add_entry("DIED","RANK",$RANK);
	   $LOG->add_entry("DIED","COUNT",$die_count);
       }
       
       my @threads = threads->list();
       foreach my $thr (@threads){
	   $thr->detach;
       }
       
       die $_[0];
   };

    #set up sharing
    share(\@chunks);
    share(\@returned_chunks);
    share(\$t_need_flag);
    share(\$t_tier);
    share(\$t_tier_result);
    share(\$t_chunk);
    share(\$t_chunk_result);
    share(\$t_terminate);
}
else{
    $main::no_threads = 1;
}

#--exec regular iprscan if no-threads and size of 1
if($size == 1 && ! $main::debug){
    exec("$FindBin::Bin/iprscan_wrap", @CMD_ARGS);
}

#--------------------------------------
#---------PRIMARY MPI PROCCESS---------
#--------------------------------------

#--check if root node
if ($rank == $root) {
    #varibles that are persistent outside of try
    my $iterator;
    my $DS_CTL;

    try{
        #---test command line  options here
        #test input files existance
        if($OPT{infile} && ! -e $OPT{infile}){
            die "ERROR: The input file \'$OPT{infile}\' does not exist.\n";
        }

        #let iprscan test all other options
        my (undef, $tfile) = tempfile(); #empty dummy test file
        my $exe = "$EXE -cli";
        my $command = "$exe " . join(' ', @ARGV);
        $command .= " -nocrc" if($OPT{nocrc});
        $command .= " -seqtype $OPT{seqtype}" if(defined $OPT{seqtype});
        $command .= " -trtable $OPT{trtable}" if(defined $OPT{trtable});
        $command .= " -goterms" if($OPT{goterms});
        $command .= " -iprlookup" if($OPT{iprlookup});
        $command .= " -format $OPT{format}" if(defined $OPT{format});
        $command .= " -verbose" if($OPT{verbose});
        $command .= " -appl " . join(" -appl ", @{$OPT{_appl}}) if(@{$OPT{_appl}});
        $command .= " -i $tfile" if($OPT{infile}); #test options on dummy file                                                                                                                                                                                       
        open(my $PAR, "$command 2>&1 |");
        my @err = <$PAR>;
        close($PAR);
        unlink($tfile);

        #report errors from iprscan
        if(! grep {/Error\: Unable to read sequence/} @err){
            foreach (@err){
                $_ =~ s/\/.*iprscan\s+\-cli/mpi_iprscan/;
            }

            die join('', @err);
        }

	#make output directory
        mkdir $OPT{out_base} if(! -d $OPT{out_base});
	my $tmplock = new File::NFSLock($OPT{out_base}."/.init_lock", 'EX', 40, 40);
        if($LOCK = new File::NFSLock($OPT{out_base}."/.gi_lock", 'SH', 40, 40)){
            $LOCK->maintain(30);
	    $OPT{_shared_id} = $LOCK->shared_id();
	    $OPT{_owners} = $LOCK->owners();
            $OPT{_multi_chpc}++ if($LOCK->owners() > 1);
            unlink($OPT{outfile}) if($OPT{_multi_chpc} && $OPT{outfile} && -e $OPT{outfile} ); #clear preexisting outfile
        }
        else{
            die "ERROR: The directory is locked.  Perhaps by another instance of mpi_iprscan.\n\n";
        }
	$tmplock->unlock() if($tmplock);

	#--send control options to all nodes
	for(my $i = 1; $i < $size; $i++){
            my $data = \%OPT;
            print "INITIALIZE OPT\t|  SND2\t|  mpi_data (ctl_opt)\t|  $rank\t-->\t$i\n" if($main::debug);
            MPI_SendII(\$data, $i, $mpi_data, MPI_COMM_WORLD);
        }

        $DS_CTL = new ds_utility(\%OPT);
        $iterator = new Iterator::Fasta($OPT{infile});
	$iterator->step($OPT{_owners});
        $iterator->skip_file($DS_CTL->{log});
    }
    catch Error::Simple with{
        my $E = shift;
        print STDERR $E->{-text};
        my $code = 2;
        $code = $E->{-value} if (defined($E->{-value}));

        exit($code);
    };
    
    #====MPI COMMUNICATION    
    #---main code for distribution of mpi data starts here    

    #thread for root node to do other things than just manage mpi
    $t_need_flag = ($main::no_threads) ? 0 : 1; #set to true for threads false for no threads
    my $thr = threads->create(\&node_thread) if($t_need_flag);

    my $go_mpi_status = 1;
    
    while($go_mpi_status){
	#====INTERNAL TIER THREAD
	#check on results from internal thread
	if (defined($t_tier_result)){
	    my $t_res = ${thaw($t_tier_result)};
	    $t_tier_result = undef;
	    $active[$root] = 0;

	    push(@failed, $t_res->{-fasta}) if ($t_res->{-failed});
	    push(@interrupted, $t_res->{-fasta}) if ($t_res->{-interrupt});
	}
	if (defined($t_chunk_result)){
	    my $chunk =  ${thaw($t_chunk_result)};
	    $t_chunk_result = undef;
	    my $id = $chunk->id();
	    ($id) = split (":", $id);
	    push (@{$c_results[$id]}, $chunk);
	    unshift (@{$res_loc[$id]}, $root);
	}
	
	#see if there are chunks to get from the internal thread
	while((@limbo_stack > 0) && (@chunks > 0) && (my $chunk = shift @chunks)){
	    my $helper = shift @limbo_stack;
	    $active[$helper] = -1 - $root; #who node now works for
	    #turn node into helper node
	    print "HELPER/RESULT REQUESTED\t|  RECV\t|  who_I_am (clr thread)\t\t|  $rank\t<--\t$helper\n" if($main::debug);
	    MPI_Recv(\$empty, 1,  MPI_INT, $helper, $who_I_am, MPI_COMM_WORLD); #clear who I am response
	    print "HELPER/RESULT REQUESTED\t|  RECV\t|  what_I_want (clr thread)\t|  $rank\t<--\t$helper\n" if($main::debug);
	    MPI_Recv(\$empty, 1, MPI_INT, $helper, $what_I_want, MPI_COMM_WORLD); #clear what I want response
	    print "HELPER/RESULT REQUESTED\t|  RECV\t|  result_status (clr thread)\t|  $rank\t<--\t$helper\n" if($main::debug);
	    MPI_Recv(\$empty, 1,  MPI_INT, $helper, $result_status, MPI_COMM_WORLD); #clear result status response
	    print "HELPER/RESULT REQUESTED\t|  SEND\t|  req_stat (wait_as_helper thread)\t|  $rank\t-->\t$helper\n" if($main::debug);
	    MPI_Send(\$wait_as_helper, 1, MPI_INT, $helper, $request_status, MPI_COMM_WORLD); #signal to become helper

	    #send chunk to helper
	    print "HELPER/RESULT REQUESTED\t|  SEND\t|  who_I_am (from thread)\t|  $rank\t-->\t$helper\n" if($main::debug);
	    MPI_Send(\$root, 1,  MPI_INT, $helper, $who_I_am, MPI_COMM_WORLD); #tell helper node I need help
	    print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_req_stat (go_chunk from thread)\t|  $rank\t-->\t$helper\n" if($main::debug);
	    MPI_Send(\$go_chunk, 1, MPI_INT, $helper, $c_req_status, MPI_COMM_WORLD ); #tell helper node a chunk is coming
	    print "HELPER/RESULT REQUESTED\t|  SND2\t|  mpi_data (chunk from thread)\t|  $rank\t-->\t$helper\n" if($main::debug);
	    MPI_SendII(\$chunk, $helper, $mpi_data, MPI_COMM_WORLD, 1); #send the chunk
	}
	
	#get tier for internal thread
	if($t_need_flag > 0){
	    my $tier;
	    my $f_count = @failed;
	    my $i_count = @interrupted;

	    #while (my $fasta = $iterator->nextFasta() || shift @failed || shift @interrupted){
	    #	$tier = Process::MpiTiers->new({fasta => $fasta,
	    #					CTL_OPT => \%OPT,
	    #					DS_CTL  => $DS_CTL,
	    #					params    => \@ARGV,
	    #					iprscan   => "$EXE -cli"},
	    #				        $root,
	    #				       'Process::IPRchunk'
	    #				       );
	    #	
	    #    last if(! $tier->terminated);
	    #}


	    
	    $tier->{_seen}++ if($i_count != @interrupted); #tag tier (easy to do it this way)
	    
	    #take a short break before processing failed contigs
	    #this handles heavy processor usage when failure is related
	    #to maker process overlap
	    sleep 1 if($f_count != @failed);

	    if(defined $tier && ! $tier->terminated){
		$t_need_flag = 0;
		my $t_val = freeze(\$tier);
		$t_tier = $t_val;
		$active[$root] = 1;
	    }
	    else{
		$t_need_flag = 2; #take tier or chunk
		$active[$root] = 0;
	    }
	}

	if($size > 1){
	    #work with mpi nodes now
	    my $who;
	    my $what;
	    my $rs_type;
	    
	    #see who asks for a file
	    print "COMM INITIALIZATION\t|  RECV\t|  who_I_am\t\t\t|  $rank\t<--\tANY\n" if($main::debug);
	    MPI_Recv(\$who, 1,  MPI_INT, -2, $who_I_am, MPI_COMM_WORLD);
	    
	    #see what the mpi node wants
	    print "COMM INITIALIZATION\t|  RECV\t|  what_I_want\t\t\t|  $rank\t<--\t$who\n" if($main::debug);
	    MPI_Recv(\$what, 1, MPI_INT, $who, $what_I_want, MPI_COMM_WORLD);
    
	    #if the node wants a tier to process, do this
	    if($what == $need_tier){
		#receive result status
		print "COMM TIER REQUESTED\t|  RECV\t|  result_status (is result?)\t|  $rank\t<--\t$who\n" if($main::debug);
		MPI_Recv(\$rs_type, 1,  MPI_INT, $who, $result_status, MPI_COMM_WORLD);
		
		#get result if available
		if($rs_type == $yes_result){
		    my $result;
		    print "COMM TIER REQUESTED\t|  RCV2\t|  mpi_data (tier_result)\t|  $rank\t<--\t$who\n" if($main::debug);
		    MPI_RecvII(\$result, $who, $mpi_data, MPI_COMM_WORLD);
		    push(@failed, $result->{-fasta}) if ($result->{-failed});
		    push(@interrupted, $result->{-fasta}) if ($result->{-interrupt});
		}
		
		#if a contig is available send tier
		my $tier;
		my $f_count = @failed;
		my $i_count = @interrupted;
		while (my $fasta = $iterator->nextFasta() || shift @failed || shift @interrupted){
		    $tier = Process::MpiTiers->new({fasta => $fasta,
						    CTL_OPT => \%OPT,
						    DS_CTL  => $DS_CTL,
						    params    => \@ARGV,
						    iprscan   => "$EXE -cli"},
						   $who,
						   'Process::IPRchunk'
						   );
		    
		    last if(! $tier->terminated);
		}
		

		$tier->{_seen}++ if(defined $tier && $i_count != @interrupted); #tag tier (easy to do it this way)		
		
		#take a short break before processing failed contigs
		#this handles heavy processor usage when failure is related
		#to maker process overlap
		sleep 1 if($f_count != @failed);
		
		if(defined $tier && ! $tier->terminated){
		    #say tier is available and send it
		    print "COMM TIER REQUESTED\t|  SEND\t|  req_stat (yes_tier)\t\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_Send(\$yes_tier, 1, MPI_INT, $who, $request_status, MPI_COMM_WORLD);
		    print "COMM TIER REQUESTED\t|  SND2\t|  mpi_data (tier)\t\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_SendII(\$tier, $who, $mpi_data, MPI_COMM_WORLD);
		    @limbo_stack = grep {$_ != $who} @limbo_stack if(defined($active[$who]) && $active[$who] == 0);
		    $active[$who] = 1;
		}
		else{
		    print "COMM TIER REQUESTED\t|  SEND\t|  req_stat (wait_ask_again)\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_Send(\$wait_ask_again, 1, MPI_INT, $who, $request_status, MPI_COMM_WORLD);
		    push(@limbo_stack, $who) if(!defined($active[$who]) || $active[$who]);
		    $active[$who] = 0;
		}
	    }
	    #if the node wants a helper or needs a chunk result, do this
	    elsif($what == $need_helper || $what == $need_c_res){
		#--first send c_res_status
		# send ids of nodes with chunk results
		if(defined ($res_loc[$who])){
		    print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_res_status (yes_c_res)\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_Send(\$yes_c_res, 1, MPI_INT, $who, $c_res_status, MPI_COMM_WORLD);
		    print "HELPER/RESULT REQUESTED\t|  SND2\t|  mpi_list (result_loc_list)\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_SendII(\$res_loc[$who], $who, $mpi_list, MPI_COMM_WORLD);
		    
		    my @locs = @{$res_loc[$who]};
		    $res_loc[$who] = undef;
		    
		    #if primary node has chunk result to send then send them
		    while (defined(my $loc = shift @locs)){
			if ($loc == $root){
			    my $res = shift @{$c_results[$who]};
			    print "COMM HAVE C_RESULT\t|  SND2\t|  mpi_data (c_res frm root)\t|  $rank\t-->\t$who\n" if($main::debug);
			    MPI_SendII(\$res, $who, $mpi_data, MPI_COMM_WORLD);
			}
		    }
		}
		#no one has anything yet
		else{
		    print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_res_status (no_c_res)\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_Send(\$no_c_res, 1, MPI_INT, $who, $c_res_status, MPI_COMM_WORLD);
		}
		
		#continue the rest if the node needs a helper
		if($what == $need_helper){
		    #find the number of helpers required
		    my $num_helpers_req;
		    print "HELPER/RESULT REQUESTED\t|  RECV\t|  work_order (num_helpers_req)\t|  $rank\t<--\t$who\n" if($main::debug);
		    MPI_Recv(\$num_helpers_req, 1, MPI_INT, $who, $work_order, MPI_COMM_WORLD);
		    
		    #number of secondary node helpers available
		    my $sec_node_avail = @limbo_stack;
		    
		    #number of primary node threads available
		    my $thr_avail = ($t_need_flag == 2 && ! defined $t_chunk) ? 1 : 0;
		    
		    #signal that no helpers are available
		    if($sec_node_avail == 0 && $thr_avail == 0){
			print "HELPER/RESULT REQUESTED\t|  SEND\t|  req_stat (no_helpers_avail)\t|  $rank\t-->\t$who\n" if($main::debug);
			MPI_Send(\$no_helper, 1, MPI_INT, $who, $request_status, MPI_COMM_WORLD);
		    }
		    else{ #if node helpers are available
			#helpers to send
			my $helpers = [];
			
			#secondary node helpers
			if($sec_node_avail > 0){
			    #seperate the helpers
			    while(@{$helpers} < $num_helpers_req && @limbo_stack > 0){
				my $helper = shift @limbo_stack;
				$active[$helper] = -1 - $who; #indicates who they work for
				push(@{$helpers}, $helper);
			    }
			    
			    $num_helpers_req -= @{$helpers};
			}
			
			#primary node thread helper
			my $root_helper_flag = 0;
			if ($thr_avail && $num_helpers_req > 0){
			    my $helper = $root;
			    $active[$root] = -1 - $who; #indicates who they work for
			    #aways make root node first
			    unshift(@{$helpers}, $helper);
			    $root_helper_flag = 1;
			}
			
			#say help is available and send ids of the helpers
			print "HELPER/RESULT REQUESTED\t|  SEND\t|  req_stat (yes_helpers_avail)\t|  $rank\t-->\t$who\n" if($main::debug);
			MPI_Send(\$yes_helper, 1, MPI_INT, $who, $request_status, MPI_COMM_WORLD);
			print "HELPER/RESULT REQUESTED\t|  SND2\t|  mpi_list (helper_loc_list)\t|  $rank\t-->\t$who\n" if($main::debug);
			MPI_SendII(\$helpers, $who, $mpi_list, MPI_COMM_WORLD);
			
			#take chunk as a helper
			if($root_helper_flag){
			    #see who's one who needs help
			    my $who2;
			    print "HELPER/RESULT REQUESTED\t|  RECV\t|  who_I_am (for root)\t|  $rank\t<--\t$who\n" if($main::debug);
			    MPI_Recv(\$who2, 1,  MPI_INT, $who, $who_I_am, MPI_COMM_WORLD);
			    
			    #get go_chunk request_status
			    my $req_stat;
			    print "HELPER/RESULT REQUESTED\t|  RECV\t|  c_req_stat (is chunk? for root)\t|  $rank\t<--\t$who2\n" if($main::debug);
			    MPI_Recv(\$req_stat, 1, MPI_INT, $who2, $c_req_status, MPI_COMM_WORLD );
			    if ($req_stat == $go_chunk){
				#get the chunk
				my $chnk;
				print "HELPER/RESULT REQUESTED\t|  RCV2\t|  mpi_data (chunk for root)\t|  $rank\t<--\t$who2\n" if($main::debug);
				MPI_RecvII(\$chnk, $who2, $mpi_data, MPI_COMM_WORLD, 1);
				$t_need_flag = 0;
				$t_chunk = $chnk;
			    }
			    elsif($req_stat == $reset){
				$t_need_flag = 2;
			    }
			    else{
				die "ERROR: Logic error in getting chunk as a helper\n";
			    }
			}

			#signal to limbo nodes to become a helper
			foreach my $helper (@$helpers){
			    next if($helper == $root); #skip root

			    #turn node into helper node
			    print "HELPER/RESULT REQUESTED\t|  RECV\t|  who_I_am (clr nd)\t\t|  $rank\t<--\t$helper\n" if($main::debug);
			    MPI_Recv(\$empty, 1,  MPI_INT, $helper, $who_I_am, MPI_COMM_WORLD); #clear who I am response
			    print "HELPER/RESULT REQUESTED\t|  RECV\t|  what_I_want (clr nd)\t\t|  $rank\t<--\t$helper\n" if($main::debug);
			    MPI_Recv(\$empty, 1, MPI_INT, $helper, $what_I_want, MPI_COMM_WORLD); #clear what I want response
			    print "HELPER/RESULT REQUESTED\t|  RECV\t|  result_status (clr nd)\t|  $rank\t<--\t$helper\n" if($main::debug);
			    MPI_Recv(\$empty, 1,  MPI_INT, $helper, $result_status, MPI_COMM_WORLD); #clear result status response
			    print "HELPER/RESULT REQUESTED\t|  SEND\t|  req_stat (wait_as_helper nd)\t|  $rank\t-->\t$helper\n" if($main::debug);
			    MPI_Send(\$wait_as_helper, 1, MPI_INT, $helper, $request_status, MPI_COMM_WORLD); #signal to become helper
			}
		    }
		}
	    }
	    #if the node has a chunk result, do this
	    elsif($what == $have_c_res){
	       #extra hanshaking to avoid non-blocking send in hydra MVAPICH2
	       MPI_Send(\$made_note, 1, MPI_INT, $who, $request_status, MPI_COMM_WORLD);

		#get the owner of the result
		my $owner;
		print "COMM HAVE C_RESULT\t|  RECV\t|  work_order (res own fr root)\t|  $rank\t<--\t$who\n" if($main::debug);
		MPI_Recv(\$owner, 1, MPI_INT, $who, $work_order, MPI_COMM_WORLD);

		if($owner == $root){ #if root is owner get result
		    my $chunk_res;
		    print "COMM HAVE C_RESULT\t|  RCV2\t|  mpi_data (c_res for root)\t|  $rank\t<--\t$who\n" if($main::debug);
		    MPI_RecvII(\$chunk_res, $who, $mpi_data, MPI_COMM_WORLD, 1); #get chunk result
		    print "COMM HAVE C_RESULT\t|  SEND\t|  who_I_am (for reset from root)\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_Send(\$root, 1,  MPI_INT, $who, $who_I_am, MPI_COMM_WORLD); #tell helper node who I am
		    print "COMM HAVE C_RESULT\t|  SEND\t|  c_req_stat (reset from root)\t|  $rank\t-->\t$who\n" if($main::debug);
		    MPI_Send(\$reset, 1, MPI_INT, $who, $c_req_status, MPI_COMM_WORLD); #reset node (no chunk)
		    push(@returned_chunks, $chunk_res);
		}
		else{ #take note of owner to tell him he has a result waiting
		    push(@{$res_loc[$owner]}, $who);
		}
	    }
	    #if what the node wants is something else
	    else{
		die "ERROR: Invalid request type\n";
	    }
	}
	
	#see if all contigs are finished
	$go_mpi_status = 0;
	if(! $iterator->finished || @failed > 0 || @interrupted > 0){
	    $go_mpi_status = 1;
	}
	else{
	    foreach my $n (@active){
		if((defined($n) && $n != 0)){
		    $go_mpi_status = 1;
		    last;
		}
	    }
	}
    }
    
    #---tell mpi nodes to terminate
    for(my $i = 1; $i < $size; $i++){
	print "TERMINATION\t|  RECV\t|  who_I_am (clr)\t\t|  $rank\t<--\t$i\n" if($main::debug);
	MPI_Recv(\$empty, 1,  MPI_INT, $i, $who_I_am, MPI_COMM_WORLD); #clear who I am response
	print "TERMINATION\t|  RECV\t|  what_I_want (clr)\t|  $rank\t<--\t$i\n" if($main::debug);
	MPI_Recv(\$empty, 1, MPI_INT, $i, $what_I_want, MPI_COMM_WORLD); #clear what I want response
	print "TERMINATION\t|  RECV\t|  result_status (clr)\t|  $rank\t<--\t$i\n" if($main::debug);
	MPI_Recv(\$empty, 1,  MPI_INT, $i, $result_status, MPI_COMM_WORLD); #clear result status response
	print "TERMINATION\t|  SEND\t|  req_stat (terminate)\t|  $rank\t-->\t$i\n" if($main::debug);
	MPI_Send(\$terminate, 1, MPI_INT, $i, $request_status, MPI_COMM_WORLD); #signal to become helper
    }
    
    print STDERR "\n\nmpi_iprscan is now finished!!!\n\n" unless($main::qq);
}
#------SECONDARY MPI PROCESSES------
else {
    my $go_mpi_status = 1;
    my $tier_result;
    my @tiers; #tier buffer (multiple level tiers)
    my $chunk_result;
    my $be_helper;

    #--receive control files from root
    my $OPT;
    if($rank <= 4){ #only first 4 so as not to overdo it
       print "INITIALIZE OPT\t|  RCV2\t|  mpi_data (ctl_opt)\t|  $rank\t<--\t$root\n" if($main::debug);
       MPI_RecvII(\$OPT, $root, $mpi_data, MPI_COMM_WORLD);

       #---set up blast databases and indexes for analyisis
       $OPT->{_not_root} = 1;
    }

    #set to true for threads false for no threads
    $t_need_flag = ($main::no_threads) ? 0 : 1;
    my $thr = threads->create(\&node_thread) if($t_need_flag);    

    while ($go_mpi_status) {
	#====INTERNAL CHUNK THREAD
	#check on results from internal thread
	if (defined($t_chunk_result)){
	    my $c_res =  ${thaw($t_chunk_result)};
	    $t_chunk_result = undef;
	    $tier->update_chunk($c_res);
	    $tier->run();
	    next;
	}
	#====END THREAD
	
	#decide what this node needs
	my $what;
	my $chunk;

	if(defined $chunk_result){
	    #NOTE: $be_helper is set to true here
	    $what = $have_c_res;
	}
        elsif($be_helper && (my $ftier = terminated(\@tiers))){
	   if(update_chunk(\@tiers, $ftier)){
	      run(\@tiers, $rank);
	      next;
	   }
	   else{
	      #NOTE: $be_helper is set to true here
	      $what = $have_c_res;
	      $chunk_result = $ftier;
	   }
        }
	elsif($be_helper && !@tiers){
	    #see who needs help
	    my $who;
	    print "HELPER/RESULT REQUESTED\t|  RECV\t|  who_I_am (for no-root)\t|  $rank\t<--\tANY\n" if($main::debug);
	    MPI_Recv(\$who, 1,  MPI_INT, -2, $who_I_am, MPI_COMM_WORLD);

	    #get request_status for chunk (was chunk available?)
	    my $chunk_status;
	    print "HELPER/RESULT REQUESTED\t|  RECV\t|  c_req_stat (is_chunk? no-root)\t|  $rank\t<--\t$who\n" if($main::debug);
	    MPI_Recv(\$chunk_status, 1, MPI_INT, $who, $C_req_status, MPI_COMM_WORLD );

	    #if there is a chunk do this
	    if($chunk_status == $go_chunk){
		#get chunk to process
		my $chnk;
		print "HELPER/RESULT REQUESTED\t|  RCV2\t|  mpi_data (chunk for no-root)\t|  $rank\t<--\t$who\n" if($main::debug);
		MPI_RecvII(\$chnk, $who, $mpi_data, MPI_COMM_WORLD);

		#if chunk is tier, treat as such
                if(ref($chnk) eq 'Process::MpiTiers'){
                   push(@tiers, $chnk);
                   run(\@tiers, $rank);
                   next;
                }

		#run chunk
		$chnk->run($rank);
		$chunk_result = $chnk;
		next;
	    }
	    #if the reset signal is received do this
	    elsif($chunk_status == $c_reset){
		$be_helper = 0;
		next;
	    }
	    else{
	       die "ERROR: Invalid chunk status signal\n;";
            }
	 }
        elsif(!$be_helper && !@tiers){
	   $what = $need_tier;
        }
        elsif(!$be_helper && (my $tier = terminated(\@tiers))){
	   if(@tiers){
	      update_chunk(\@tiers, $tier);
	      run(\@tiers, $rank);
	      next;
	   }
	   else{
	      $what = $need_tier;

	      #collect errors and failures if any
	      $tier_result->{-error} = $tier->error;
	      $tier_result->{-failed} = $tier->failed;
	      $tier_result->{-interrupt} = $tier->interrupt if(!$tier->{_seen});
	      $tier_result->{-fasta} = $tier->fasta if($tier->failed || $tier_result->{-interrupt});
	      $tier = undef;
	   }
        }
        elsif((chunk_total_count(\@tiers) == 1) && (num_chunks(\@tiers) == 1)){
	   #run lonesome chunk (should never happen but it's here just in case)
	   my $r_chunk = next_chunk(\@tiers); #running chunk
	   $chunk = undef; #just incase

            #if chunk is tier, treat as such
	   if(ref($r_chunk) eq 'Process::MpiTiers'){
	      push(@tiers, $r_chunk);
	      run(\@tiers, $rank);
	      next;
	   }

	   $r_chunk->run($rank);
	   update_chunk(\@tiers, $r_chunk);
	   run(\@tiers, $rank);
	   next;
        }
	elsif((result_count(\@tiers) == chunk_total_count(\@tiers) - 1) && (num_chunks(\@tiers) == 1)){
            #run last chunk outside of thread
	   my $r_chunk = next_chunk(\@tiers); #running chunk
	   $chunk = undef; #just incase

            #if chunk is tier, treat as such
	   if(ref($r_chunk) eq 'Process::MpiTiers'){
	      push(@tiers, $r_chunk);
	      run(\@tiers, $rank);
	      next;
	   }

	   $r_chunk->run($rank);
	   update_chunk(\@tiers, $r_chunk);
	   run(\@tiers, $rank);
	   next;
        }
        elsif((result_count(\@tiers) == chunk_total_count(\@tiers) - 1) && (num_chunks(\@tiers) == 1)){
            #run last chunk outside of thread
	   my $r_chunk = next_chunk(\@tiers); #running chunk
	   $chunk = undef; #just incase

            #if chunk is tier, treat as such
	   if(ref($r_chunk) eq 'Process::MpiTiers'){
	      push(@tiers, $r_chunk);
	      run(\@tiers, $rank);
	      next;
	   }

	   $r_chunk->run($rank);
	   update_chunk(\@tiers, $r_chunk);
	   run(\@tiers, $rank);
	   next;
        }
        elsif((result_count(\@tiers) == chunk_total_count(\@tiers) - 1) && (num_chunks(\@tiers) == 1)){
	   #run last chunk outside of thread
	   my $r_chunk = next_chunk(\@tiers); #running chunk
	   $chunk = undef; #just incase
	   
            #if chunk is tier, treat as such
	   if(ref($r_chunk) eq 'Process::MpiTiers'){
	      push(@tiers, $r_chunk);
	      run(\@tiers, $rank);
	      next;
	   }

	   $r_chunk->run($rank);
	   update_chunk(\@tiers, $r_chunk);
	   run(\@tiers, $rank);
	   next;
        }
        elsif($thr && !$t_need_flag && (result_count(\@tiers) == chunk_total_count(\@tiers) - 1)){
            #waiting on thread
	   sleep 1;
	   next;
        }
        elsif((num_chunks(\@tiers) > 1) || ($thr && !$t_need_flag && num_chunks(\@tiers) > 0)){
	   $chunk = next_chunk(\@tiers) if(!$thr || $t_need_flag);

            #if chunk is tier, treat as such
	   if(ref($chunk) eq 'Process::MpiTiers'){
	      push(@tiers, $chunk);
	      run(\@tiers, $rank);
	      next;
	   }

	   $what = $need_helper;
        }
        elsif(result_count(\@tiers) < chunk_total_count(\@tiers)){
	   $chunk = next_chunk(\@tiers) if(!$thr || $t_need_flag);

            #if chunk is tier, treat as such
	   if(ref($chunk) eq 'Process::MpiTiers'){
	      push(@tiers, $chunk);
	      run(\@tiers, $rank);
	      next;
	   }

	   $what = $need_c_res;
        }
        else{
	   run(\@tiers, $rank);
	   next;
        }

	#tell the  primary process what node it is speaking to
	print "COMM INITIALIZATION\t|  SEND\t|  who_I_am\t\t\t|  $rank\t-->\t$root\n" if($main::debug);
	MPI_Send(\$rank, 1, MPI_INT, $root, $who_I_am, MPI_COMM_WORLD );
		
	#--tell primary node what this node needs
	print "COMM INITIALIZATION\t|  SEND\t|  what_I_want\t\t\t|  $rank\t-->\t$root\n" if($main::debug);
	MPI_Send(\$what, 1, MPI_INT, $root, $what_I_want, MPI_COMM_WORLD );
	
	#if what I want is a tier do this
	if($what == $need_tier){
	    #Send result status
	    my $rs_type = (defined($tier_result)) ? $yes_result: $no_result;
	    my $stat = (defined($tier_result)) ? "yes_result": "no_result" if($main::debug);
	    print "COMM TIER REQUESTED\t|  SEND\t|  result_status ($stat)\t|  $rank\t-->\t$root\n" if($main::debug);
	    MPI_Send(\$rs_type, 1, MPI_INT, $root, $result_status, MPI_COMM_WORLD );
	    
	    #Send result if available
	    if($rs_type == $yes_result){
		print "COMM TIER REQUESTED\t|  SND2\t|  mpi_data (tier_result)\t|  $rank\t-->\t$root\n" if($main::debug);
		MPI_SendII(\$tier_result, $root, $mpi_data, MPI_COMM_WORLD);
		$tier_result = undef;
	    }
	    
	    #get request_status for the tier
	    my $req_status;
	    print "COMM TIER REQUESTED\t|  RECV\t|  req_stat (is tier?)\t\t|  $rank\t<--\t$root\n" if($main::debug);
	    MPI_Recv(\$req_status, 1, MPI_INT, $root, $request_status, MPI_COMM_WORLD);
	    
	    #get tier and run if it if there is one
	    if($req_status == $yes_tier){
	       my $tier;
               print "COMM TIER REQUESTED\t|  RCV2\t|  mpi_data (tier)\t\t|  $rank\t<--\t$root\n" if($main::debug);
               MPI_RecvII(\$tier, $root, $mpi_data, MPI_COMM_WORLD);
               push(@tiers, $tier);
               run(\@tiers, $rank);
               next;
	    }
	    #just wait and then try again later
	    elsif($req_status == $wait_ask_again){
		sleep 1;
		next;
	    }
	    #reset signal received
	    elsif($req_status == $reset){
		$be_helper = 0;
		next;
	    }
	    #wait as helper if asked to (blocks with MPI_Recv)
	    elsif($req_status == $wait_as_helper){
		$be_helper = 1;
		next
	    }
	    #termination signal received
	    elsif($req_status == $terminate){
		$go_mpi_status = 0;
                last;
	    }
	    else{
		die "ERROR: Invalid request status type\n";
	    }

	    next;
	} #if what I want is help or the result from a helper do this
	elsif ($what == $need_helper || $what == $need_c_res){
	    #give chunk to thread
	    if($thr && defined($chunk) && $tier->chunk_total_count > 1){
		$t_need_flag = 0;
		$t_chunk = freeze(\$chunk);
		$chunk = undef;
	    }

	    #check c_result_status
	    my $c_res_stat;
	    print "COMM HAVE C_RESULT\t|  RECV\t|  c_res_status (is c_res?)\t|  $rank\t<--\t$root\n" if($main::debug);
	    MPI_Recv(\$c_res_stat, 1, MPI_INT, $root, $c_res_status, MPI_COMM_WORLD);
	    
	    #if there are chunk results, do this
	    my $locs = [];
	    if($c_res_stat == $yes_c_res){
		#get ids of nodes with chunk result
		print "HELPER/RESULT REQUESTED\t|  RCV2\t|  mpi_list (result_loc_list)\t|  $rank\t<--\t$root\n" if($main::debug);
		MPI_RecvII(\$locs, $root, $mpi_list, MPI_COMM_WORLD);
		
		#get chunk results from only the root node for now
		my @non;
		foreach my $loc (@{$locs}){
		    if ($loc != $root){
			push(@non, $loc);
			next;
		    }
		    my $c_res;
		    print "COMM HAVE C_RESULT\t|  RCV2\t|  mpi_data (c_res frm root)\t|  $rank\t<--\t$root\n" if($main::debug);
		    MPI_RecvII(\$c_res, $loc, $mpi_data, MPI_COMM_WORLD);

                    update_chunk(\@tiers, $c_res);
                    actualize(\@tiers, $rank);
		}
		$locs = \@non;
	    }
	    
	    #send off chunks if the node needs a helper
	    my $helpers = [];
	    if ($what == $need_helper){
		#send the number of helpers required
		my $num_helpers_req = ($tier->num_chunks - @$locs > 0) ? $tier->num_chunks - @$locs : 0;		
		print "HELPER/RESULT REQUESTED\t|  SEND\t|  work_order (num_helpers_req)\t|  $rank\t-->\t$root\n" if($main::debug);
		MPI_Send(\$num_helpers_req, 1, MPI_INT, $root, $work_order, MPI_COMM_WORLD);
		
		#see if helper is available
		my $help_stat;
		print "HELPER/RESULT REQUESTED\t|  RECV\t|  req_stat (is helper avail?)\t|  $rank\t<--\t$root\n" if($main::debug);
		MPI_Recv(\$help_stat, 1, MPI_INT, $root, $request_status, MPI_COMM_WORLD);
		
		if($help_stat == $yes_helper){
		    print "HELPER/RESULT REQUESTED\t|  RCV2\t|  mpi_list (helper_loc_list)\t|  $rank\t<--\t$root\n" if($main::debug);
		    MPI_RecvII(\$helpers, $root, $mpi_list, MPI_COMM_WORLD); #get helper ids
		}
	    }

            #send chunk to helper (root first)
	    foreach my $helper (@{$helpers}){
	       foreach my $helper (sort {$a <=> $b} @{$helpers}){
		  print "HELPER/RESULT REQUESTED\t|  SEND\t|  who_I_am (for *helpers)\t|  $rank\t-->\t$helper\n" if($main::debug);
		  MPI_Send(\$rank, 1,  MPI_INT, $helper, $who_I_am, MPI_COMM_WORLD); #say I'm the one who needs help

                #send chunk if availabile
                my $chnk = next_chunk(\@tiers, 1);
                if($chnk && ($helper != 0 || (ref($chnk) ne 'Process::MpiTiers'))){
		    print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_req_stat (go_chunk *helpers)\t|  $rank\t-->\t$helper\n" if($main::debug);
		    MPI_Send(\$go_chunk, 1, MPI_INT, $helper, $c_req_status, MPI_COMM_WORLD); #say chunk is available
		    print "HELPER/RESULT REQUESTED\t|  SND2\t|  mpi_data (chunk *helpers)\t|  $rank\t-->\t$helper\n" if($main::debug);
		    MPI_SendII(\$chnk, $helper, $mpi_data, MPI_COMM_WORLD); #send chunk
		}
		else{
                   push(@tiers, $chnk) if($chnk);
                   print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_req_stat (reset *helpers)\t|  $rank\t-->\t$helper\n" if($main::debug);
                   MPI_Send(\$c_reset, 1, MPI_INT, $helper, $c_req_status, MPI_COMM_WORLD); #send reset signal
                   actualize(\@tiers, $rank);
		}
	    }

	    #since root communication has terminated, get chunk results from non-root nodes
	    #and send them something else to do or release them
	    foreach my $loc (@{$locs}){
		#this shouldn't happen but just incase
		die "ERROR: Root has chunk result post-root communication" if ($loc == $root);

		my $c_res;
		print "COMM HAVE C_RESULT\t|  RCV2\t|  mpi_data (c_res no-root)\t|  $rank\t<--\t$loc\n" if($main::debug);
                MPI_RecvII(\$c_res, $loc, $mpi_data, MPI_COMM_WORLD);
                update_chunk(\@tiers, $c_res);
                actualize(\@tiers, $rank);

		print "HELPER/RESULT REQUESTED\t|  SEND\t|  who_I_am (for xhelpers)\t|  $rank\t-->\t$loc\n" if($main::debug);
		MPI_Send(\$rank, 1,  MPI_INT, $loc, $who_I_am, MPI_COMM_WORLD); #restablish relationship

		#send something else or release helper
		if($tier->num_chunks > 0){
		    my $chnk = $tier->next_chunk;
		    print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_req_stat (go_chunk xhelpers)\t|  $rank\t-->\t$loc\n" if($main::debug);
                    MPI_Send(\$go_chunk, 1, MPI_INT, $loc, $c_req_status, MPI_COMM_WORLD); #say chunk is available
		    print "HELPER/RESULT REQUESTED\t|  SND2\t|  mpi_data (chunk xhelpers)\t|  $rank\t-->\t$loc\n" if($main::debug);
                    MPI_SendII(\$chnk, $loc, $mpi_data, MPI_COMM_WORLD); #send chunk
		}
		else{
		    print "HELPER/RESULT REQUESTED\t|  SEND\t|  c_req_stat (reset xhelpers)\t|  $rank\t-->\t$loc\n" if($main::debug);
                    MPI_Send(\$reset, 1, MPI_INT, $loc, $c_req_status, MPI_COMM_WORLD); #send reset signal
		}
	    }

	    #nothing provided and nothing to advance to (wait)
	    if(!@{$helpers} && !@{$locs} && !$chunk && !$t_need_flag ){
		sleep 1;
	    }

	    #finally run local chunk if there is one
	    if(defined($chunk)){
		my $r_chunk = $chunk; #running chunk
		$chunk = undef;
		$r_chunk->run($rank);
                update_chunk(\@tiers,$r_chunk);
                actualize(\@tiers, $rank);
	    }

	    run(\@tiers, $rank);
	    next;
	}
        #if just finished a helper chunk, inform that it is finished
	    elsif($what == $have_c_res){
            #extra handshaking to handle non-blocking send in hydra MPIAVCH2
	       my $req_status;
	       print "COMM HAVE C_RESULT\t|  RECV\t|  req_stat (arrived?)\t\t|  $rank\t<--\t$root\n" if($main::debug);
	       MPI_Recv(\$req_status, 1, MPI_INT, $root, $request_status, MPI_COMM_WORLD);

	       if($req_status ne $made_note){
		  die "ERROR: Invalid request_status type\n";
	       }

            #send the owner id of the result
	       my $owner = $chunk_result->id();
	       ($owner) = split(":", $owner);
	       print "COMM HAVE C_RESULT\t|  SEND\t|  work_order (res own to root)\t|  $rank\t-->\t$root\n" if($main::debug);
	       MPI_Send(\$owner, 1, MPI_INT, $root, $work_order, MPI_COMM_WORLD);

            #send the result
	       print "COMM HAVE C_RESULT\t|  SND2\t|  mpi_data (c_res frm no-root)\t|  $rank\t-->\t$owner\n" if($main::debug);
	       MPI_SendII(\$chunk_result, $owner, $mpi_data, MPI_COMM_WORLD);
	       $chunk_result = undef;

	       next;
	    }
	 }
}

#---------ALL NODES----------
#---release thread
$t_terminate = 1; #signals to thread to clean up
unless($main::no_threads){
    my @threads = threads->list();
    foreach my $thr (@threads){
	$thr->kill('KILL')->join();
    }
}

$LOCK()->unlock() if($LOCK()); #remove global lock
MPI_Finalize();	#terminate MPI

exit(0);

#-----------------------------------------------------------------------------
#----------------------------------- SUBS ------------------------------------
#-----------------------------------------------------------------------------
#other things for root node to do
#(thread allows root to process tiers like a secondary node)
sub node_thread {
   my $tier;
   my $chunk;

   if($main::no_threads){ #pause and return if no thread is needed
       $t_need_flag = 0;
       return;
   }

   #set up thread signal to exit gracefully
   $SIG{'KILL'} = sub { threads->exit(); };

   $t_need_flag = 1;

   while(! $t_terminate){
      #load serialized tier into tier
      if(! defined ($tier) && defined ($t_tier)){
	 $t_need_flag = 0;
	 $tier = ${thaw($t_tier)};
	 $t_tier = undef;
	 next;
      }#process tier
      elsif(defined($tier)){
	 #get chunk results from other nodes
	 while(my $res = shift @returned_chunks){
	    $res = ${thaw($res)};
	    $tier->update_chunk($res);
	 }

	 #run the tier as far as possible
	 $tier->run;

	 #get all chunks available
	 my $chnk = $tier->next_chunk;
	 while(my $o_chnk = $tier->next_chunk){
	    $o_chnk = freeze(\$o_chnk);
	    push (@chunks, $o_chnk);
	 }

	 #run chunks one at a time
	 $chnk->run($rank) if ($chnk);
	 $tier->update_chunk($chnk) if ($chnk);
	 while($chnk = shift @chunks){
	    $chnk = ${thaw($chnk)};
	    if($tier->failed){ #skip chunks after failure
		$tier->update_chunk($chnk);
		next;
	    }

	    $chnk->run($rank);
	    $tier->update_chunk($chnk);
	 }

	 #let tier advance if possible
	 $tier->run();

	 #terminate tier, wait, or continue
	 if($tier->terminated){
	    my $tier_result;
	    $tier_result->{-error} = $tier->error;
	    $tier_result->{-failed} = $tier->failed;
	    $tier_result->{-interrupt} = $tier->interrupt if(!$tier->{_seen});
	    $tier_result->{-fasta} = $tier->fasta if ($tier->failed || $tier_result->{-interrupt});

	    sleep 1 while (defined ($t_tier_result)); #pause incase result will be overwritten
	    $t_tier_result = freeze(\$tier_result);
	    $tier = undef;
	    $t_need_flag = 1;
	 }#take a break
	 elsif($tier->num_chunks == 0){
	    #keeps thread from hogging resources while waiting for external results
	    sleep 1;
	 }

	 next;
      }#load serialized chunk into chunk
      elsif(! defined ($chunk) && defined ($t_chunk)){
	 $t_need_flag = 0;
	 $chunk = ${thaw($t_chunk)};
	 $t_chunk = undef;
	 next;
      }#process chunk
      elsif(defined($chunk)){
	 $chunk->run($rank);
	 sleep 1 while (defined ($t_chunk_result)); #pause incase result will be overwritten
	 $t_chunk_result = freeze(\$chunk);
	 $chunk = undef;
	 $t_need_flag = 1;
	 next;
      }#take a break
      else{
	 #keeps thread form hogging resources when there is nothing to do
	 sleep 1;
      }
   }
}
#----------------------------------------------------------------------------
#easy dump of string to a tempfile
sub totemp{
   my $data = shift @_;

   my ($fh, $name) = tempfile();
   print $fh $data;
   close ($fh);

   return $name;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer and return the first MpiChunk found
sub next_chunk {
   my $buffer = shift;

   for(my $i = @$buffer - 1; $i >= 0; $i--){
      my $chunk = $buffer->[$i]->next_chunk();
      return $chunk if($chunk);
   }

   return undef;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer and returns first finished tier
sub terminated {
   my $buffer = shift;

   my @keep;
   while (my $t = shift @$buffer){
      if($t->terminated){
	 unshift(@$buffer, @keep);
	 return $t;
      }
      else{
	 push(@keep, $t);
      }
   }

   @$buffer = @keep;
   return;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer and return the first MpiChunk found
sub update_chunk {
   my $buffer = shift;
   my $chunk = shift;;

   my $stat;
   foreach my $t (@$buffer){
      if($t->id eq $chunk->parent){
	 $t->update_chunk($chunk);
	 $stat = 1;
	 last;
      }
   }

   return $stat;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer and count all total chunks
sub chunk_total_count {
   my $buffer = shift;

   actualize($buffer);

   my $sum = 0;
   foreach my $t (@$buffer){
      $sum += $t->chunk_total_count();
   }

   return $sum;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer andcount results
sub result_count {
   my $buffer = shift;

   my $sum = 0;
   foreach my $t (@$buffer){
      $sum += $t->result_count();
   }

   return $sum;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer and update the correct one
sub num_chunks {
   my $buffer = shift;

   actualize($buffer);

   my $sum = 0;
   foreach my $t (@$buffer){
      $sum += $t->num_chunks();
   }

   return $sum;
}
#----------------------------------------------------------------------------
#function to itterate through chunk buffer to adavance without run
sub actualize {
   my $buffer = shift;
   my $rank = shift;

   foreach my $t (@$buffer){
      $t->actualize($rank);
   }
}
#----------------------------------------------------------------------------
#function to itterate through buffer to run
sub run {
   my $buffer = shift;
   my $rank = shift;

   actualize($buffer, $rank);

   if(chunk_total_count($buffer) > 1){
      return;
   }

   while ((chunk_total_count($buffer) == 1) && (my $chunk = next_chunk($buffer))){
      if(ref($chunk) eq 'Process::MpiTiers'){
	 push(@$buffer, $chunk);
	 return run($buffer, $rank);
      }
      else{
	 $chunk->run($rank);
	 update_chunk($buffer, $chunk);
      }
   }

   return;
}
#----------------------------------------------------------------------------
#make MPI_Send work with unsafe signaling
sub MPI_Send {
    my @args = @_;
    UNSAFE_SIGNALS {
	&Parallel::MPIcar::MPI_Send(@args);
    }
}
#----------------------------------------------------------------------------
#make MPI_Recv work with unsafe signaling
sub MPI_Recv {
    my @args = @_;
    UNSAFE_SIGNALS {
	&Parallel::MPIcar::MPI_Recv(@args);
    }
}
#----------------------------------------------------------------------------
#sends scalar variable contents via serialization
#scalar can hold ref to other data structures
sub MPI_SendII{
   my $msg = shift @_;
   my $target = shift @_;
   my $tag = shift @_;
   my $communicator = shift @_;
   my $no_freeze = shift @_;

   my $send = ($no_freeze) ? $msg : \ (freeze($msg));
   my $length = length($$send);
   
   MPI_Send(\$length, 1, MPI_INT, $target, $message_length, $communicator);
   MPI_Send($send, $length, MPI_CHAR, $target, $tag, $communicator);   
}
#----------------------------------------------------------------------------
#receives serialized scalar variable
#scalar can hold ref to other data structures
sub MPI_RecvII{
   my $ref = shift @_;
   my $source = shift @_;
   my $tag = shift @_;
   my $communicator = shift @_;
   my $no_thaw = shift @_;

   my $recv;
   my $length;

   MPI_Recv(\$length, 1, MPI_INT, $source, $message_length, $communicator);
   MPI_Recv(\$recv, $length, MPI_CHAR, $source, $tag, $communicator); #receive line

   ${$ref} = ($no_thaw) ? $recv : ${thaw($recv)};
}
#----------------------------------------------------------------------------
#initialize the MPI communication
sub MPI_Init{
   my @args = @_;
   UNSAFE_SIGNALS {
      &Parallel::MPIcar::MPI_Init(@args);
   }
}
#----------------------------------------------------------------------------
#terminate MPI communication
sub MPI_Finalize{
   my @args = @_;
   UNSAFE_SIGNALS {
      &Parallel::MPIcar::MPI_Finalize(@args);
   }
}
